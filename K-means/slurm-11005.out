Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/11/17 00:32:41 INFO SparkContext: Running Spark version 2.0.0
16/11/17 00:32:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/11/17 00:32:41 INFO SecurityManager: Changing view acls to: vpinnaka
16/11/17 00:32:41 INFO SecurityManager: Changing modify acls to: vpinnaka
16/11/17 00:32:41 INFO SecurityManager: Changing view acls groups to: 
16/11/17 00:32:41 INFO SecurityManager: Changing modify acls groups to: 
16/11/17 00:32:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(vpinnaka); groups with view permissions: Set(); users  with modify permissions: Set(vpinnaka); groups with modify permissions: Set()
16/11/17 00:32:42 INFO Utils: Successfully started service 'sparkDriver' on port 32775.
16/11/17 00:32:42 INFO SparkEnv: Registering MapOutputTracker
16/11/17 00:32:42 INFO SparkEnv: Registering BlockManagerMaster
16/11/17 00:32:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-000800bb-29d8-4c4d-9a8c-3b595e312c43
16/11/17 00:32:42 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
16/11/17 00:32:42 INFO SparkEnv: Registering OutputCommitCoordinator
16/11/17 00:32:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/11/17 00:32:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.18.3.85:4040
16/11/17 00:32:42 INFO SparkContext: Added JAR target/first-example-1.0-SNAPSHOT.jar at spark://172.18.3.85:32775/jars/first-example-1.0-SNAPSHOT.jar with timestamp 1479364362552
16/11/17 00:32:42 INFO Executor: Starting executor ID driver on host localhost
16/11/17 00:32:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38787.
16/11/17 00:32:42 INFO NettyBlockTransferService: Server created on 172.18.3.85:38787
16/11/17 00:32:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.18.3.85, 38787)
16/11/17 00:32:42 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.3.85:38787 with 2004.6 MB RAM, BlockManagerId(driver, 172.18.3.85, 38787)
16/11/17 00:32:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.18.3.85, 38787)
16/11/17 00:32:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 128.0 KB, free 2004.5 MB)
16/11/17 00:32:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 2004.5 MB)
16/11/17 00:32:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.3.85:38787 (size: 14.6 KB, free: 2004.6 MB)
16/11/17 00:32:43 INFO SparkContext: Created broadcast 0 from textFile at JavaKMeansExample.java:38
16/11/17 00:32:43 INFO FileInputFormat: Total input paths to process : 1
16/11/17 00:32:43 INFO SparkContext: Starting job: takeSample at KMeans.scala:386
16/11/17 00:32:43 INFO DAGScheduler: Got job 0 (takeSample at KMeans.scala:386) with 39 output partitions
16/11/17 00:32:43 INFO DAGScheduler: Final stage: ResultStage 0 (takeSample at KMeans.scala:386)
16/11/17 00:32:43 INFO DAGScheduler: Parents of final stage: List()
16/11/17 00:32:43 INFO DAGScheduler: Missing parents: List()
16/11/17 00:32:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:216), which has no missing parents
16/11/17 00:32:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 2004.5 MB)
16/11/17 00:32:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 2004.5 MB)
16/11/17 00:32:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.3.85:38787 (size: 2.5 KB, free: 2004.6 MB)
16/11/17 00:32:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
16/11/17 00:32:43 INFO DAGScheduler: Submitting 39 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:216)
16/11/17 00:32:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 39 tasks
16/11/17 00:32:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5788 bytes)
16/11/17 00:32:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/11/17 00:32:43 INFO Executor: Fetching spark://172.18.3.85:32775/jars/first-example-1.0-SNAPSHOT.jar with timestamp 1479364362552
16/11/17 00:32:43 INFO TransportClientFactory: Successfully created connection to /172.18.3.85:32775 after 66 ms (0 ms spent in bootstraps)
16/11/17 00:32:43 INFO Utils: Fetching spark://172.18.3.85:32775/jars/first-example-1.0-SNAPSHOT.jar to /tmp/spark-1ff37eda-5870-4aed-8a3f-742583d59717/userFiles-b86682b0-cc95-4d59-adb9-570002cf5a0c/fetchFileTemp6792486800923651985.tmp
16/11/17 00:32:43 INFO Executor: Adding file:/tmp/spark-1ff37eda-5870-4aed-8a3f-742583d59717/userFiles-b86682b0-cc95-4d59-adb9-570002cf5a0c/first-example-1.0-SNAPSHOT.jar to class loader
16/11/17 00:32:43 INFO HadoopRDD: Input split: file:/home/vpinnaka/HDFS-K-means/K-means/Phones_accelerometer.txt:0+33554432
16/11/17 00:32:43 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/11/17 00:32:43 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/11/17 00:32:43 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/11/17 00:32:43 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/11/17 00:32:43 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/11/17 00:32:43 WARN BlockManager: Putting block rdd_2_0 failed
16/11/17 00:32:43 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 3
	at my.spark.JavaKMeansExample$1.call(JavaKMeansExample.java:51)
	at my.spark.JavaKMeansExample$1.call(JavaKMeansExample.java:40)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:214)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/11/17 00:32:43 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5788 bytes)
16/11/17 00:32:43 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/11/17 00:32:43 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 3
	at my.spark.JavaKMeansExample$1.call(JavaKMeansExample.java:51)
	at my.spark.JavaKMeansExample$1.call(JavaKMeansExample.java:40)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:214)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/11/17 00:32:43 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
16/11/17 00:32:43 INFO HadoopRDD: Input split: file:/home/vpinnaka/HDFS-K-means/K-means/Phones_accelerometer.txt:33554432+33554432
16/11/17 00:32:43 INFO TaskSchedulerImpl: Cancelling stage 0
16/11/17 00:32:43 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1)
16/11/17 00:32:43 INFO TaskSchedulerImpl: Stage 0 was cancelled
16/11/17 00:32:43 INFO DAGScheduler: ResultStage 0 (takeSample at KMeans.scala:386) failed in 0.291 s
16/11/17 00:32:43 INFO DAGScheduler: Job 0 failed: takeSample at KMeans.scala:386, took 0.377067 s
16/11/17 00:32:43 WARN BlockManager: Putting block rdd_2_1 failed
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 3
	at my.spark.JavaKMeansExample$1.call(JavaKMeansExample.java:51)
	at my.spark.JavaKMeansExample$1.call(JavaKMeansExample.java:40)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:214)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1115)
	at org.apache.spark.rdd.RDD$$anonfun$takeSample$1.apply(RDD.scala:545)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.takeSample(RDD.scala:534)
	at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:386)
	at org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:256)
	at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:219)
	at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:201)
	at org.apache.spark.mllib.clustering.KMeans$.train(KMeans.scala:539)
	at org.apache.spark.mllib.clustering.KMeans$.train(KMeans.scala:550)
	at org.apache.spark.mllib.clustering.KMeans.train(KMeans.scala)
	at my.spark.JavaKMeansExample.main(JavaKMeansExample.java:195)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 3
	at my.spark.JavaKMeansExample$1.call(JavaKMeansExample.java:51)
	at my.spark.JavaKMeansExample$1.call(JavaKMeansExample.java:40)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:214)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/11/17 00:32:43 INFO Executor: Executor killed task 1.0 in stage 0.0 (TID 1)
16/11/17 00:32:43 INFO SparkContext: Invoking stop() from shutdown hook
16/11/17 00:32:43 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost): TaskKilled (killed intentionally)
16/11/17 00:32:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/11/17 00:32:43 INFO SparkUI: Stopped Spark web UI at http://172.18.3.85:4040
16/11/17 00:32:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/11/17 00:32:43 INFO MemoryStore: MemoryStore cleared
16/11/17 00:32:43 INFO BlockManager: BlockManager stopped
16/11/17 00:32:43 INFO BlockManagerMaster: BlockManagerMaster stopped
16/11/17 00:32:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/11/17 00:32:43 INFO SparkContext: Successfully stopped SparkContext
16/11/17 00:32:43 INFO ShutdownHookManager: Shutdown hook called
16/11/17 00:32:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-1ff37eda-5870-4aed-8a3f-742583d59717
